so will need three models

and three pufs 

the actual input mapping for each of them will prolly be different 

for results need all the graphs

and maybe explain what goes into the input mapping for each one

which basically determines how secure the PUFS

start with arbiter puf for LR, then perceptron, and then nerual network 

then move to next one bsacially just chaning the input mapping


-------------------------------------------------------


Okay so 

Lets get working graphs and metrics for each model first 

LR
perceptron
NN

Once that works get them for the none and arbiter so we have comparisons

# also make file system after the graphs work so the folders are organized i guess

Next we haae two pufs left
the main things to actually change is jsut the input mapping if eveyrhting else was done correctly


Start with Interpose I guess

- im guessing it will be bad for most of the models 

Then somehting else
------------------------------
Extra not really extra add the README 
    - for readme if u are running make sure to have folders premade cuz i didn't put check for it 
also push this on github
MAYBE SET THE LIMITS OF THE AXES 
right now we save th models but don't do anything with them maybe if we want to have like a tester script i guess
also manually changing the numbers for different models rn

maybe add noise to training pufs??

lightweight secure puf
or feed forward xor
maybe no interpose?
okay so arbiter, xor, lightweight, then mention that there are more and harder 

stuff to complete:
also might have to change input mapping for lightweight 

and then get all graphs and make presentation


Results:
evertyhing bad for none

everything bad for Lightweight
BUTTT, if you put it on 4 bits lol the NN does work, but you know nowadays its 128 and 64 

lr is good for arbiter

xor is good for arbiter 

nn is good for arbiter, xor 

okay i proved 128 is still good for xor of NN slightly worse accurcay tho

having too many crps 1000000, makes it just take forever not even improving,
if it was not working initally more data won't help 

k is But higher k also means:

More area and power.

Slightly higher probability of unstable outputs (needs careful hardware calibration).

okay so 1 chain is just arbiter puf?????? correct make it no linear

even with 2 chains perceptron just doesn't have capacity to learn for XOR

oh crap it fails on 8 ???,6 not working either, will a million crps save it tho??, if not just try 2 for proof it works

okay lets just add noise, still keeps it the same 

even with 0.5 noise on arbiter LR, still accurate just less confidence so still good 

noise works with k=2 for NN on XOR

time to check lightweight
idk on 64 bits, k 2 chains, no noise, lightweight is still kinda bad on NN maybe check the input mapping,
not sure but might have to be quits